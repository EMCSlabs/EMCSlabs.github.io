---
layout: post
title: RNN part1
description: RNN basics
headline: RNN basics
categories: ML
tags: 
  - ML
comments: false
mathjax: true
featured: false
published: true
---

## RNN part1 - RNN basics

RNN과 LSTM에 대한 개념 설명과 세부 알고리즘에 대한 내용은 이미 문서화가 잘 되어 있으므로, References를 참고할 것. 

ANN에 대한 기본 지식을 전제로 작성함.

LSTM및 TensorFlow코드로의 응용은 추후 포스팅에 포함할 예정.

---
###1. 기본 개념
- 뉴럴 네트워크에서는 기본적으로 인풋과 히든레이어 그리고 아웃풋이 한 방향으로 연결되어 있음 (e.g. 인풋 --> 히든레이어 --> 아웃풋). 하나의 example (e.g. 고양이 사진)은 하나의 아웃풋 (e.g. '고양이')과 연결되어 있지만, 다음의 인풋은 이전의 인풋과 전혀 연관되어있지 않음. 예를 들어, 이번 iteration에 넣는 example (e.g. 고양이 사진1)과 다음 iteration에 넣는 example (e.g. 고양이 사진2)은 독립사건임. 하지만, 인풋의 순서가 중요한 데이터들도 존재 (e.g. 조음데이터, 경제동향추세 등). RNN은 인풋 example을 어떤 순서로 넣느냐에 따라서 패러미터가 달리 훈련되고, 전혀 다른 결과를 만들어냄. 결론은 히든레이어에 self-loop을 붙였보자는 것. 좀 더 정확히 말하자면, **hidden activation**끼리 연결해보자는 것. 
-  hidden activation을 연결한다는 것과 backpropagation할 때 몇 개 텀을 추가한다는 것만 제외하면 나머지 구조는 유사함. 물론 여러 architecture로 확장가능하다는 점에서 그리고 RNN의 한계를 보완한다는 점에서 (e.g. LSTM, Bidirectional LSTM, Echo State Network 등 - 추후 포스팅에서 논의) 특히 RNN기반의 LSTM이 주목받고 있음.

###2. 알고리즘
####- Feedfoward
- H = sigmoid(Wx + b)와 같이 hidden activation(H)은 웨이트(W)와 인풋(x)의 곱에 바이어스(b)를 더한 뒤, non-linear transformation을 시켜주는 기본 원리를 바탕으로 함
- 시간 개념(t)을 넣고, hidden activation에 이전의 hidden activation이 연결되어 있다는 점을 반영하여 식을 정리하면 H<sub>t</sub> = sigmoid(W<sub>ih</sub>x<sub>t</sub> + W<sub>hh</sub>H<sub>t-1</sub> + b_h)
	- W<sub>ih</sub>: 인풋에서 hidden으로 가는 웨이트
	- W<sub>hh</sub>: hidden에서 hidden으로 가는 웨이트
	- H<sub>t-1</sub>: 이전의 hidden activation
	- b_h: 현 시점의 hidden activation에 더해지는 바이어스
- 중요한 점은, W<sub>ih</sub>과 W<sub>hh</sub> 그리고 바이어스는 각 시점에서 모두 동일하다는 것. t시점에서의 {W<sub>ih</sub>, W<sub>hh</sub>, 바이어스}와 t-1시점의 {W<sub>ih</sub>, W<sub>hh</sub>, 바이어스}는 모두 동일함. 이렇게 함으로써 패러미터의 숫자를 엄청나게 늘리지 않아도 되고, 패러미터를 업데이트 하기에도 용이해짐
- RNN의 구조를 graph chart라고 생각할 때, 각 레이어에 어떤 선들이 연결되어 있고, 어디로 뻗어나가는지만 잘 추적한다면, Feedfoward는 크게 어려운 부분은 아닌듯
- 크게 두 개의 Task를 정의할 때, classification과 regression의 차이는 아웃풋 레이어의 구성 (e.g. 1-of-K coding)과 output function (non-linear or linear function)의 사용에 따라 차이가 나고, 이는 RNN에서도 동일함

####- Backpropagation
- Cost가 패러미터(W, b)에 대하여 얼마만큼 변화하는지를 계산하여 (i.e. <sup>∂C</sup>/<sub>∂W</sub>, <sup>∂C</sup>/<sub>∂b</sub>), 에러만큼을 패러미터에 반영하고 수정하는 과정이 backpropagation임
- RNN에서도 backpropagation은 동일하게 적용되는데, 단 주의할 점은 하나의 레이어에 복수의 레이어가 연결되어 있는 경우가 있기 때문에, 에러를 역전파할 때 양 방향 혹은 복수의 방향에 다 전파해야한다는 점 (예를 들어, 특정 시점(t)의 히든레이어는 아웃풋과 이후 히든레이어(t+1)가 동시에 연결되어 있으므로, 이전 히든레이어(t-1)와 인풋 방향으로 backpropagation을 할 때에, (t+1)시점의 히든레이어의 영향도 생각해주어야함
- 모든 패러미터가 모든 시점에 대하여 동일하므로 (i.e. W<sub>t</sub>==W<sub>t-1</sub>, b<sub>t</sub>==b<sub>t-1</sub>), 각 시점에서의 에러를 쌓아놓았다가 나중에 한꺼번에 패러미터 업데이트에 반영하는 것이 기본 원리 (패러미터가 모든 시점에 다 영향을 주기 때문에 적절한 learning rate를 사용하는 것이 중요함. 그래서 Adam, Adagrad, Adadelta처럼 learning rate가 패러미터 각각에 대하여 선택적으로, 그리고 적절히 변하게 하는 최적화 방식을 사용하는 것이 중요해진듯 - [AdamOptimizer포스팅](https://github.com/EMCSlabs/emcslabs.github.io/blob/master/_posts/2016-12-15-AdamOptimizer.md) 및 [Andrej Karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 참조)



###3. 코드 예제
대표예제 몇 가지만 첨부
####- Classification
- [Anyone can learn to code](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/): 2진법 덧셈을 계산기가 아닌 RNN이 배우도록 학습시킨 단순한 학습모형. RNN의 feedforward/backpropagation 과정을 코드레벨에서 확인하고 이해하기 가장 좋은 듯. 파이선 기반. ([backpropagation](https://goo.gl/OIvnCF)은 링크에 추가로 정리함)
- [Karpathy simple LM](https://gist.github.com/karpathy/d4dee566867f8291f086): charcater 수준의 단순한 RNN기반의 LM. Adagrad, gradient checking등이 포함되어 있음.
- [Youngsun's Repo](https://youngsunhere.github.io/phoneme-level-rnn/): 한글를 자모 단위로 쪼개서 RNN을 훈련함 (Karpathy코드 기반). 

####- Regression
- 생각보다 regression용으로 베포된 RNN코드를 찾기가 힘듬. Classification기반의 예시 RNN 코드가 많은 편. sinewave를 인풋/아웃풋으로 두고 RNN을 학습시킨 예제 코드 참조 ([rnn\_sinewave\_test](https://goo.gl/qqkUjQ): Karpathy코드 기반으로 작성. 이미 훈련된 패러미터 기반으로 여러 테스트를 해봄).

###4. 생각해볼거리
- [Regression 코드](https://goo.gl/qqkUjQ)에서는 frequency=10hz이고 노이즈가 추가된 sinewave를 훈련시켜서 이를 테스트셋에 적용해봄. 결과를 살펴보면, RNN은 smoother의 역할을 하는 것으로 보임(smoothing에 영향을 주는 요소는?). frequency가 10보다 작거나, 10보다 큰 데이터를 새로운 인풋으로 주면 아웃풋은 새로운 frequency에 맞추어 나옴 (데이터 전체의 특징보다 local pattern을 학습하는 것의 의미는?). 1로 이루어진 수직선 (1,1,1,...,1)을 인풋으로 주면 damping의 효과 같은 것이 관찰됨 (이전의 기억이 얼마나 멀리 적용하는지를 수치화 할 수 있는 지표가 될까?)



---
###References:
* [Anyone can learn to code](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)
* [Andrej Karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
* [Understanding LSTM Networks - colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
	
<p align="right"> Jaekoo Kang <p>